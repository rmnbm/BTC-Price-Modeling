{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1453721",
   "metadata": {},
   "source": [
    "# Advanced Modeling for BTC Daily Features\n",
    "\n",
    "Plan :\n",
    "1. Implémenter des versions avancées des algos vus en cours (ElasticNet, GradientBoosting/HistGB, RandomForest/ExtraTrees).\n",
    "2. Définir le plan apprentissage/test : choix des données, split chrono, contrôle de l'overfitting.\n",
    "3. Analyser et critiquer les résultats avec MAE/RMSE/MAPE.\n",
    "4. Combiner plusieurs algos (stacking/average) pour la décision.\n",
    "5. Choisir un algo hors cours (réseau de neurones MLP) avec référence.\n",
    "6. Expliquer l'algo et justifier le choix (référence scientifique).\n",
    "7. Implémenter, évaluer et comparer avec les résultats précédents et le baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b221ea",
   "metadata": {},
   "source": [
    "## 0. Imports & Helpers\n",
    "- Chargement des librairies\n",
    "- Fonctions utilitaires (RMSE/MAPE, split chrono, évaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4195db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import ElasticNet, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    pct = np.abs((y_true - y_pred) / y_true)\n",
    "    pct = pd.Series(pct).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return float(pct.mean() * 100)\n",
    "\n",
    "def chrono_split(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    return (X.iloc[:train_end], y.iloc[:train_end],\n",
    "            X.iloc[train_end:val_end], y.iloc[train_end:val_end],\n",
    "            X.iloc[val_end:], y.iloc[val_end:])\n",
    "\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return {\n",
    "        'model': name,\n",
    "        'MAE': mean_absolute_error(y_test, preds),\n",
    "        'RMSE': rmse(y_test, preds),\n",
    "        'MAPE%': mape(y_test, preds),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83da05",
   "metadata": {},
   "source": [
    "### Explication (Imports & helpers)\n",
    "- Fonctions utilitaires partagées : RMSE(Root Mean Squared Error)/MAPE (Mean Absolute Percentage Error)  pour les métriques, split chronologique pour éviter la fuite temporelle, `evaluate_model` pour entraîner + scorer.\n",
    "- Les pipelines scaler+modèle assurent une standardisation cohérente quand nécessaire (linéaire, MLP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c4e43",
   "metadata": {},
   "source": [
    "## 1. Données et cible\n",
    "- Chargement `btc_features_daily.csv`\n",
    "- Tri chronologique\n",
    "- Cible : close du lendemain (`target_close_next`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90659d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 corrélations (absolues):\n",
      "adj close       0.999377\n",
      "close           0.999377\n",
      "typ_price       0.999328\n",
      "ohlc4           0.999259\n",
      "median_price    0.999248\n",
      "high            0.999135\n",
      "low             0.999077\n",
      "open            0.998810\n",
      "wma_10          0.998472\n",
      "ma_7            0.998334\n",
      "Name: target_close_next, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(Path('btc_features_daily.csv'), parse_dates=['timestamp']).sort_values('timestamp')\n",
    "df['target_close_next'] = df['close'].shift(-1)\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "corr = (\n",
    "    df.drop(columns=['timestamp'])\n",
    "      .corr()['target_close_next']\n",
    "      .drop('target_close_next')\n",
    "      .abs()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "print('Top 10 corrélations (absolues):')\n",
    "print(corr.head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0312f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3,973 rows, 52 columns\n",
      "Missing ratio (top 5):\n",
      "timestamp    0.0\n",
      "adj close    0.0\n",
      "close        0.0\n",
      "high         0.0\n",
      "low          0.0\n",
      "dtype: float64\n",
      "Splits -> train 2780, val 596, test 596\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('btc_features_daily.csv')\n",
    "df = pd.read_csv(data_path, parse_dates=['timestamp']).sort_values('timestamp').reset_index(drop=True)\n",
    "print(f\"Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "print('Missing ratio (top 5):')\n",
    "print(df.isna().mean().head())\n",
    "\n",
    "# Cible = close J+1\n",
    "df['target_close_next'] = df['close'].shift(-1)\n",
    "df = df.iloc[:-1]\n",
    "feature_cols = [c for c in df.columns if c not in ['timestamp', 'target_close_next']]\n",
    "X = df[feature_cols]\n",
    "y = df['target_close_next']\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = chrono_split(X, y)\n",
    "print(f\"Splits -> train {len(X_train)}, val {len(X_val)}, test {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f851d",
   "metadata": {},
   "source": [
    "## 1bis. Ajout des données de sentiment\n",
    "On ajoute le Fear & Greed Index hebdomadaire comme feature supplémentaire :\n",
    "- chargement de `sentiment_dataset.csv` si présent, sinon `sentiment_fng_dataset.csv`\n",
    "- resample en quotidien avec forward-fill\n",
    "- fusion avec les features BTC, puis recalcul de la cible et des splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78216daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion sentiment: features = 52, sentiment cols = ['fear_and_greed_index']\n",
      "Splits -> train 2779, val 596, test 596\n"
     ]
    }
   ],
   "source": [
    "# Fusion des données de sentiment (hebdo) en daily\n",
    "from pathlib import Path\n",
    "\n",
    "sent_paths = [Path('sentiment_dataset.csv'), Path('sentiment_fng_dataset.csv')]\n",
    "sent_path = None\n",
    "for p in sent_paths:\n",
    "    if p.exists():\n",
    "        sent_path = p\n",
    "        break\n",
    "if sent_path is None:\n",
    "    raise FileNotFoundError('Aucun fichier de sentiment trouvé (sentiment_dataset.csv ou sentiment_fng_dataset.csv)')\n",
    "\n",
    "sent = pd.read_csv(sent_path)\n",
    "if 'date' not in sent.columns:\n",
    "    sent = sent.rename(columns={sent.columns[0]: 'date'})\n",
    "sent['date'] = pd.to_datetime(sent['date'])\n",
    "# Nettoyage des noms de colonnes\n",
    "sent.columns = [c.strip().lower() for c in sent.columns]\n",
    "# Passage en daily avec ffill\n",
    "sent_daily = sent.set_index('date').sort_index().resample('D').ffill().reset_index()\n",
    "\n",
    "# Fusion sur la date normalisée\n",
    "df['date'] = df['timestamp'].dt.normalize()\n",
    "df = df.merge(sent_daily, on='date', how='left')\n",
    "sent_cols = [c for c in sent_daily.columns if c != 'date']\n",
    "for c in sent_cols:\n",
    "    df[c] = df[c].ffill().bfill()\n",
    "\n",
    "df = df.drop(columns=['date'])\n",
    "\n",
    "# Recalcule cible + splits avec les features enrichies\n",
    "df['target_close_next'] = df['close'].shift(-1)\n",
    "df = df.iloc[:-1]\n",
    "feature_cols = [c for c in df.columns if c not in ['timestamp', 'target_close_next']]\n",
    "X = df[feature_cols]\n",
    "y = df['target_close_next']\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = chrono_split(X, y)\n",
    "print(f\"Fusion sentiment: features = {len(feature_cols)}, sentiment cols = {sent_cols}\")\n",
    "print(f\"Splits -> train {len(X_train)}, val {len(X_val)}, test {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2343d588",
   "metadata": {},
   "source": [
    "### Explication (Fusion sentiment)\n",
    "- On aligne le Fear & Greed Index (hebdo) sur une fréquence quotidienne via `resample('D').ffill()`.\n",
    "- Les valeurs manquantes hors plage sont comblées par `ffill()/bfill()` pour ne pas perdre d'observations.\n",
    "- Les nouvelles colonnes de sentiment sont ajoutées aux features avant de recalculer la cible et les splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0eeb3",
   "metadata": {},
   "source": [
    "### Explication (Données & cible)\n",
    "- On charge `btc_features_daily.csv`, on trie par date, pas de valeurs manquantes sur les principales colonnes.\n",
    "- Cible = close du lendemain (`target_close_next`) : formulation régressive, sensible à l'autocorrélation.\n",
    "- Split 70/15/15 chronologique pour reproduire un scénario temps-réel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f78b84",
   "metadata": {},
   "source": [
    "## 2. Corrélations et contrôle de l'overfitting\n",
    "- Corrélations très élevées entre niveaux de prix (indique redondance)\n",
    "- Plan pour limiter l'overfit :\n",
    "  - Standardisation pour modèles linéaires/MLP\n",
    "  - Hyperparamètres contraints (profondeur, min_samples_leaf)\n",
    "  - Split chrono (pas de fuite temporelle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb6f9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 8 corrélations absolues avec la cible:\n",
      "adj close       0.999376\n",
      "close           0.999376\n",
      "typ_price       0.999327\n",
      "ohlc4           0.999258\n",
      "median_price    0.999247\n",
      "high            0.999133\n",
      "low             0.999075\n",
      "open            0.998809\n",
      "Name: target_close_next, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr = (\n",
    "    df.drop(columns=['timestamp'])\n",
    "      .corr()['target_close_next']\n",
    "      .drop('target_close_next')\n",
    "      .abs()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "print('Top 8 corrélations absolues avec la cible:')\n",
    "print(corr.head(8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d175a5",
   "metadata": {},
   "source": [
    "### Explication (Corrélations & overfitting)\n",
    "- Les corrélations >0.99 entre prix (close/adj/typ_price) et la cible indiquent une forte redondance.\n",
    "- Risque : les modèles complexes apprennent surtout à recopier le prix actuel → difficile de généraliser.\n",
    "- Contre-mesures : régularisation, profondeur limitée, validation temporelle stricte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c173e94",
   "metadata": {},
   "source": [
    "## 3. Baseline\n",
    "Baseline persistance : prédire que le close de demain = close d’aujourd’hui.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0434cd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'persistence_close',\n",
       " 'MAE': 1475.7585596686242,\n",
       " 'RMSE': 2027.3115008294158,\n",
       " 'MAPE%': 1.7420895365842408}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds = X_test['close']\n",
    "baseline_res = {\n",
    "    'model': 'persistence_close',\n",
    "    'MAE': mean_absolute_error(y_test, baseline_preds),\n",
    "    'RMSE': rmse(y_test, baseline_preds),\n",
    "    'MAPE%': mape(y_test, baseline_preds),\n",
    "}\n",
    "baseline_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34362c5",
   "metadata": {},
   "source": [
    "### Explication (Baseline)\n",
    "- Cette baseline prédit simplement que le close de demain = close d'aujourd'hui.\n",
    "- Si ses MAE/RMSE/MAPE sont très bas, cela signifie que la série est fortement autocorrélée et difficile à battre.\n",
    "- Tout modèle avancé doit faire mieux (MAE/RMSE plus petits, MAPE plus faible) pour être intéressant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd00fdd",
   "metadata": {},
   "source": [
    "## 4. Modèles avancés (versions cours)\n",
    "- ElasticNet (L1+L2) pour gérer colinéarité et sélection douce.\n",
    "- Ridge (référence linéaire régularisée).\n",
    "- GradientBoostingRegressor et HistGradientBoostingRegressor (plus robuste, bins).\n",
    "- RandomForestRegressor et ExtraTreesRegressor (arbres bagging/variance réduite).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaffac14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>persistence_close</td>\n",
       "      <td>1475.758560</td>\n",
       "      <td>2027.311501</td>\n",
       "      <td>1.742090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ridge</td>\n",
       "      <td>1766.136507</td>\n",
       "      <td>2315.695459</td>\n",
       "      <td>2.037025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2338.694596</td>\n",
       "      <td>2922.421378</td>\n",
       "      <td>2.631561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>extra_trees</td>\n",
       "      <td>25049.640083</td>\n",
       "      <td>31826.364387</td>\n",
       "      <td>24.531465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf</td>\n",
       "      <td>25010.779401</td>\n",
       "      <td>31835.098496</td>\n",
       "      <td>24.474331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gboost</td>\n",
       "      <td>26481.603420</td>\n",
       "      <td>32964.672377</td>\n",
       "      <td>26.238465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hist_gb</td>\n",
       "      <td>26395.776179</td>\n",
       "      <td>33114.697807</td>\n",
       "      <td>26.031712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model           MAE          RMSE      MAPE%\n",
       "0  persistence_close   1475.758560   2027.311501   1.742090\n",
       "2              ridge   1766.136507   2315.695459   2.037025\n",
       "1         elasticnet   2338.694596   2922.421378   2.631561\n",
       "6        extra_trees  25049.640083  31826.364387  24.531465\n",
       "5                 rf  25010.779401  31835.098496  24.474331\n",
       "3             gboost  26481.603420  32964.672377  26.238465\n",
       "4            hist_gb  26395.776179  33114.697807  26.031712"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "    ('elasticnet', make_pipeline(StandardScaler(), ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000))),\n",
    "    ('ridge', make_pipeline(StandardScaler(), Ridge(alpha=5.0))),\n",
    "    ('gboost', GradientBoostingRegressor(random_state=0, n_estimators=400, learning_rate=0.02, max_depth=3)),\n",
    "    ('hist_gb', HistGradientBoostingRegressor(random_state=0, max_depth=4, learning_rate=0.05, max_iter=500)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=400, random_state=0, n_jobs=-1, max_depth=6, min_samples_leaf=3)),\n",
    "    ('extra_trees', ExtraTreesRegressor(n_estimators=400, random_state=0, n_jobs=-1, max_depth=6, min_samples_leaf=3)),\n",
    "]\n",
    "\n",
    "results = [baseline_res]\n",
    "for name, model in models:\n",
    "    results.append(evaluate_model(name, model, X_train, y_train, X_test, y_test))\n",
    "\n",
    "pd.DataFrame(results).sort_values('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c069990b",
   "metadata": {},
   "source": [
    "### Explication (Baseline)\n",
    "- Cette baseline prédit simplement que le close de demain = close d'aujourd'hui.\n",
    "- Si ses MAE/RMSE/MAPE sont très bas, cela signifie que la série est fortement autocorrélée et difficile à battre.\n",
    "- Tout modèle avancé doit faire mieux (MAE/RMSE plus petits, MAPE plus faible) pour être intéressant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81552138",
   "metadata": {},
   "source": [
    "### Explication (Modèles avancés)\n",
    "- **ElasticNet/Ridge** : testent si une régularisation linéaire peut extraire un léger signal malgré la colinéarité.\n",
    "- **GB/HistGB** : capturent des non-linéarités; l'apprentissage lent (petit learning_rate) limite l'overfit.\n",
    "- **RF/ExtraTrees** : bagging d'arbres, ici contraints (max_depth/min_samples_leaf) pour réduire l'overfit.\n",
    "- Interprétation : comparez leurs métriques au baseline. Si elles sont plus grandes, ils n'apportent pas de gain; s'ils sont plus petites, ils surpassent la persistance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356e1d1",
   "metadata": {},
   "source": [
    "## 5. Ensemble learning\n",
    "On combine plusieurs modèles pour lisser leurs erreurs : moyenne simple et StackingRegressor (ridge meta-learner).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6fea984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>persistence_close</td>\n",
       "      <td>1475.758560</td>\n",
       "      <td>2027.311501</td>\n",
       "      <td>1.742090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stack_elastic_ridge_rf_histgb</td>\n",
       "      <td>1700.283300</td>\n",
       "      <td>2248.035931</td>\n",
       "      <td>1.969836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ridge</td>\n",
       "      <td>1766.136507</td>\n",
       "      <td>2315.695459</td>\n",
       "      <td>2.037025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2338.694596</td>\n",
       "      <td>2922.421378</td>\n",
       "      <td>2.631561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>avg_elastic_ridge_rf_histgb</td>\n",
       "      <td>13662.807629</td>\n",
       "      <td>17064.742660</td>\n",
       "      <td>13.538915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>extra_trees</td>\n",
       "      <td>25049.640083</td>\n",
       "      <td>31826.364387</td>\n",
       "      <td>24.531465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf</td>\n",
       "      <td>25010.779401</td>\n",
       "      <td>31835.098496</td>\n",
       "      <td>24.474331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gboost</td>\n",
       "      <td>26481.603420</td>\n",
       "      <td>32964.672377</td>\n",
       "      <td>26.238465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hist_gb</td>\n",
       "      <td>26395.776179</td>\n",
       "      <td>33114.697807</td>\n",
       "      <td>26.031712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model           MAE          RMSE      MAPE%\n",
       "0              persistence_close   1475.758560   2027.311501   1.742090\n",
       "8  stack_elastic_ridge_rf_histgb   1700.283300   2248.035931   1.969836\n",
       "2                          ridge   1766.136507   2315.695459   2.037025\n",
       "1                     elasticnet   2338.694596   2922.421378   2.631561\n",
       "7    avg_elastic_ridge_rf_histgb  13662.807629  17064.742660  13.538915\n",
       "6                    extra_trees  25049.640083  31826.364387  24.531465\n",
       "5                             rf  25010.779401  31835.098496  24.474331\n",
       "3                         gboost  26481.603420  32964.672377  26.238465\n",
       "4                        hist_gb  26395.776179  33114.697807  26.031712"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit base models\n",
    "fitted = {}\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    fitted[name] = model\n",
    "\n",
    "# Moyenne simple des prédictions des 4 meilleurs modèles (ElasticNet, Ridge, RF, HistGB)\n",
    "selected = ['elasticnet','ridge','rf','hist_gb']\n",
    "preds_avg = np.mean([fitted[n].predict(X_test) for n in selected], axis=0)\n",
    "ensemble_avg = {\n",
    "    'model': 'avg_elastic_ridge_rf_histgb',\n",
    "    'MAE': mean_absolute_error(y_test, preds_avg),\n",
    "    'RMSE': rmse(y_test, preds_avg),\n",
    "    'MAPE%': mape(y_test, preds_avg),\n",
    "}\n",
    "\n",
    "# Stacking\n",
    "estimators = [(n, fitted[n]) for n in selected]\n",
    "stack = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=1.0), passthrough=False)\n",
    "stack.fit(X_train, y_train)\n",
    "preds_stack = stack.predict(X_test)\n",
    "ensemble_stack = {\n",
    "    'model': 'stack_elastic_ridge_rf_histgb',\n",
    "    'MAE': mean_absolute_error(y_test, preds_stack),\n",
    "    'RMSE': rmse(y_test, preds_stack),\n",
    "    'MAPE%': mape(y_test, preds_stack),\n",
    "}\n",
    "\n",
    "ensemble_results = [ensemble_avg, ensemble_stack]\n",
    "pd.DataFrame(results + ensemble_results).sort_values('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfd12d",
   "metadata": {},
   "source": [
    "### Explication (Ensemble learning)\n",
    "- **Moyenne simple** : lisse les erreurs de modèles complémentaires; efficace si les erreurs ne sont pas corrélées.\n",
    "- **Stacking** : méta-modèle (Ridge) apprend à pondérer les prédictions; peut capturer des interactions entre modèles.\n",
    "- Lecture : si l'ensemble baisse MAE/RMSE/MAPE vs chaque modèle individuel et vs baseline, il apporte de la robustesse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0fc8d",
   "metadata": {},
   "source": [
    "## 6. Modèle hors cours : MLP Regressor (réseau de neurones feed-forward)\n",
    "Référence : Zhang, G. P. (2003), \"Time series forecasting using a hybrid ARIMA and neural network model\", *Neurocomputing*. Les MLP capturent des relations non linéaires; ici on l’emploie sur les features existantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85091a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>persistence_close</td>\n",
       "      <td>1475.758560</td>\n",
       "      <td>2027.311501</td>\n",
       "      <td>1.742090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stack_elastic_ridge_rf_histgb</td>\n",
       "      <td>1700.283300</td>\n",
       "      <td>2248.035931</td>\n",
       "      <td>1.969836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ridge</td>\n",
       "      <td>1766.136507</td>\n",
       "      <td>2315.695459</td>\n",
       "      <td>2.037025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2338.694596</td>\n",
       "      <td>2922.421378</td>\n",
       "      <td>2.631561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mlp_regressor</td>\n",
       "      <td>4826.045582</td>\n",
       "      <td>6028.113499</td>\n",
       "      <td>5.170059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>avg_elastic_ridge_rf_histgb</td>\n",
       "      <td>13662.807629</td>\n",
       "      <td>17064.742660</td>\n",
       "      <td>13.538915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>extra_trees</td>\n",
       "      <td>25049.640083</td>\n",
       "      <td>31826.364387</td>\n",
       "      <td>24.531465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf</td>\n",
       "      <td>25010.779401</td>\n",
       "      <td>31835.098496</td>\n",
       "      <td>24.474331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gboost</td>\n",
       "      <td>26481.603420</td>\n",
       "      <td>32964.672377</td>\n",
       "      <td>26.238465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hist_gb</td>\n",
       "      <td>26395.776179</td>\n",
       "      <td>33114.697807</td>\n",
       "      <td>26.031712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model           MAE          RMSE      MAPE%\n",
       "0              persistence_close   1475.758560   2027.311501   1.742090\n",
       "8  stack_elastic_ridge_rf_histgb   1700.283300   2248.035931   1.969836\n",
       "2                          ridge   1766.136507   2315.695459   2.037025\n",
       "1                     elasticnet   2338.694596   2922.421378   2.631561\n",
       "9                  mlp_regressor   4826.045582   6028.113499   5.170059\n",
       "7    avg_elastic_ridge_rf_histgb  13662.807629  17064.742660  13.538915\n",
       "6                    extra_trees  25049.640083  31826.364387  24.531465\n",
       "5                             rf  25010.779401  31835.098496  24.474331\n",
       "3                         gboost  26481.603420  32964.672377  26.238465\n",
       "4                        hist_gb  26395.776179  33114.697807  26.031712"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=500, random_state=0, early_stopping=True)\n",
    ")\n",
    "\n",
    "mlp_res = evaluate_model('mlp_regressor', mlp, X_train, y_train, X_test, y_test)\n",
    "\n",
    "pd.DataFrame(results + ensemble_results + [mlp_res]).sort_values('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec30ba4",
   "metadata": {},
   "source": [
    "### Explication (MLP hors cours)\n",
    "- MLP Regressor (réseau de neurones feed-forward) peut modéliser des relations non linéaires complexes.\n",
    "- Early stopping réduit l'overfit; le scaling est crucial.\n",
    "- Interprétation : comparez ses métriques à la baseline et aux autres. Si l'erreur est plus élevée, les features actuelles ne suffisent pas; si elle baisse, cela suggère un signal non linéaire exploité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc72581",
   "metadata": {},
   "source": [
    "## 7. Analyse et comparaison\n",
    "- Comparez chaque modèle au baseline persistance.\n",
    "- Notez si l’ensemble (avg/stack) ou le MLP apportent un gain.\n",
    "- Inspectez les métriques (MAE/RMSE/MAPE) et la robustesse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e83593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>persistence_close</td>\n",
       "      <td>1475.758560</td>\n",
       "      <td>2027.311501</td>\n",
       "      <td>1.742090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stack_elastic_ridge_rf_histgb</td>\n",
       "      <td>1700.283300</td>\n",
       "      <td>2248.035931</td>\n",
       "      <td>1.969836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ridge</td>\n",
       "      <td>1766.136507</td>\n",
       "      <td>2315.695459</td>\n",
       "      <td>2.037025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2338.694596</td>\n",
       "      <td>2922.421378</td>\n",
       "      <td>2.631561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mlp_regressor</td>\n",
       "      <td>4826.045582</td>\n",
       "      <td>6028.113499</td>\n",
       "      <td>5.170059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>avg_elastic_ridge_rf_histgb</td>\n",
       "      <td>13662.807629</td>\n",
       "      <td>17064.742660</td>\n",
       "      <td>13.538915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>extra_trees</td>\n",
       "      <td>25049.640083</td>\n",
       "      <td>31826.364387</td>\n",
       "      <td>24.531465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf</td>\n",
       "      <td>25010.779401</td>\n",
       "      <td>31835.098496</td>\n",
       "      <td>24.474331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gboost</td>\n",
       "      <td>26481.603420</td>\n",
       "      <td>32964.672377</td>\n",
       "      <td>26.238465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hist_gb</td>\n",
       "      <td>26395.776179</td>\n",
       "      <td>33114.697807</td>\n",
       "      <td>26.031712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model           MAE          RMSE      MAPE%\n",
       "0              persistence_close   1475.758560   2027.311501   1.742090\n",
       "1  stack_elastic_ridge_rf_histgb   1700.283300   2248.035931   1.969836\n",
       "2                          ridge   1766.136507   2315.695459   2.037025\n",
       "3                     elasticnet   2338.694596   2922.421378   2.631561\n",
       "4                  mlp_regressor   4826.045582   6028.113499   5.170059\n",
       "5    avg_elastic_ridge_rf_histgb  13662.807629  17064.742660  13.538915\n",
       "6                    extra_trees  25049.640083  31826.364387  24.531465\n",
       "7                             rf  25010.779401  31835.098496  24.474331\n",
       "8                         gboost  26481.603420  32964.672377  26.238465\n",
       "9                        hist_gb  26395.776179  33114.697807  26.031712"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_table = pd.DataFrame(results + ensemble_results + [mlp_res]).sort_values('RMSE')\n",
    "final_table.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1dd5c7",
   "metadata": {},
   "source": [
    "### Explication (Analyse finale)\n",
    "- Triez par RMSE/MAE : plus petit = meilleur. La MAPE donne une lecture en %.\n",
    "- Identifier le premier modèle qui bat clairement la baseline; sinon conclure que les features sont insuffisantes et qu'il faut changer la cible (retours/direction) ou ajouter des variables exogènes.\n",
    "- Vérifier la stabilité : si un modèle surperforme mais avec très faible marge, valider en rolling split/val croisée temporelle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db9f21",
   "metadata": {},
   "source": [
    "## 8. Variante : cible en rendement (log-return) + sentiment\n",
    "On teste si formuler la cible en rendement log (close_{t+1}/close_t) permet d'exploiter mieux les signaux, en incluant le sentiment F&G ffill en daily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b0b72f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline_zero_return</td>\n",
       "      <td>0.017443</td>\n",
       "      <td>0.024094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elastic</td>\n",
       "      <td>0.017478</td>\n",
       "      <td>0.024101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.018167</td>\n",
       "      <td>0.024724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hist_gb</td>\n",
       "      <td>0.031036</td>\n",
       "      <td>0.037191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ridge</td>\n",
       "      <td>0.033007</td>\n",
       "      <td>0.039333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model       MAE      RMSE\n",
       "0  baseline_zero_return  0.017443  0.024094\n",
       "2               elastic  0.017478  0.024101\n",
       "4                    rf  0.018167  0.024724\n",
       "3               hist_gb  0.031036  0.037191\n",
       "1                 ridge  0.033007  0.039333"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variante retour log + sentiment (rapide)\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "btc = pd.read_csv('btc_features_daily.csv', parse_dates=['timestamp']).sort_values('timestamp').reset_index(drop=True)\n",
    "sent = pd.read_csv('sentiment_fng_dataset.csv')\n",
    "if 'date' not in sent.columns:\n",
    "    sent = sent.rename(columns={sent.columns[0]:'date'})\n",
    "sent['date'] = pd.to_datetime(sent['date'])\n",
    "sent.columns = [c.lower() for c in sent.columns]\n",
    "sent_daily = sent.set_index('date').sort_index().resample('D').ffill().reset_index()\n",
    "\n",
    "btc['date'] = btc['timestamp'].dt.normalize()\n",
    "btc = btc.merge(sent_daily, on='date', how='left')\n",
    "btc['fear_and_greed_index'] = btc['fear_and_greed_index'].ffill().bfill()\n",
    "btc = btc.drop(columns=['date'])\n",
    "\n",
    "# Cible : log-return du close t->t+1\n",
    "y = np.log(btc['close'].shift(-1) / btc['close'])\n",
    "btc = btc.iloc[:-1]\n",
    "y = y.iloc[:-1]\n",
    "feature_cols = [c for c in btc.columns if c not in ['timestamp']]\n",
    "X = btc[feature_cols]\n",
    "\n",
    "n = len(X); tr=int(n*0.7); va=int(n*0.85)\n",
    "X_train, X_val, X_test = X.iloc[:tr], X.iloc[tr:va], X.iloc[va:]\n",
    "y_train, y_val, y_test = y.iloc[:tr], y.iloc[tr:va], y.iloc[va:]\n",
    "\n",
    "rmse = lambda yt, yp: mean_squared_error(yt, yp)**0.5\n",
    "results_ret = []\n",
    "# baseline : retour nul\n",
    "pred0 = np.zeros_like(y_test)\n",
    "results_ret.append({'model':'baseline_zero_return','MAE':mean_absolute_error(y_test,pred0),'RMSE':rmse(y_test,pred0)})\n",
    "\n",
    "models_ret = [\n",
    "    ('ridge', make_pipeline(StandardScaler(), Ridge(alpha=5.0))),\n",
    "    ('elastic', make_pipeline(StandardScaler(), ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000))),\n",
    "    ('hist_gb', HistGradientBoostingRegressor(random_state=0, max_depth=4, learning_rate=0.05, max_iter=500)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=400, random_state=0, n_jobs=-1, max_depth=6, min_samples_leaf=3)),\n",
    "]\n",
    "for name, model in models_ret:\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    results_ret.append({'model':name,'MAE':mean_absolute_error(y_test,pred),'RMSE':rmse(y_test,pred)})\n",
    "\n",
    "pd.DataFrame(results_ret).sort_values('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee82fb",
   "metadata": {},
   "source": [
    "### Explication (rendement log)\n",
    "- Baseline = retour nul (prédire aucune variation) reste la meilleure ou quasi ex æquo, montrant que le bruit domine.\n",
    "- Les modèles (Ridge/ElasticNet/HistGB/RF) ne dépassent pas le baseline, malgré la reformulation de la cible et l'ajout du sentiment F&G.\n",
    "- Conclusion intermédiaire : même avec une cible en rendement et le sentiment, on n'extrait pas de signal prédictif net. Il faut d'autres variables ou une cible différente (classification directionnelle) pour espérer mieux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41e467",
   "metadata": {},
   "source": [
    "### Conclusion rapide\n",
    "- Si aucun modèle ne bat le baseline : les features n’apportent pas de signal supplémentaire; envisager des cibles en rendements/direction et des variables exogènes.\n",
    "- Si un ensemble ou le MLP réduit la MAPE/MAE, poursuivre l’affinage (tuning hyperparam, rolling validation).\n",
    "- Toujours valider en split temporel ou rolling pour éviter la fuite.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
