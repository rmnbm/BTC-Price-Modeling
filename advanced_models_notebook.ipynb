{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1453721",
   "metadata": {},
   "source": [
    "# Advanced Modeling for BTC Daily Features\n",
    "\n",
    "Plan :\n",
    "1. Implémenter des versions avancées des algos vus en cours (ElasticNet, GradientBoosting/HistGB, RandomForest/ExtraTrees).\n",
    "2. Définir le plan apprentissage/test : choix des données, split chrono, contrôle de l'overfitting.\n",
    "3. Analyser et critiquer les résultats avec MAE/RMSE/MAPE.\n",
    "4. Combiner plusieurs algos (stacking/average) pour la décision.\n",
    "5. Choisir un algo hors cours (réseau de neurones MLP) avec référence.\n",
    "6. Expliquer l'algo et justifier le choix (référence scientifique).\n",
    "7. Implémenter, évaluer et comparer avec les résultats précédents et le baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b221ea",
   "metadata": {},
   "source": [
    "## 0. Imports & Helpers\n",
    "- Chargement des librairies\n",
    "- Fonctions utilitaires (RMSE/MAPE, split chrono, évaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4195db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import ElasticNet, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    pct = np.abs((y_true - y_pred) / y_true)\n",
    "    pct = pd.Series(pct).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return float(pct.mean() * 100)\n",
    "\n",
    "def chrono_split(X, y, train_ratio=0.7, val_ratio=0.15):\n",
    "    n = len(X)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    return (X.iloc[:train_end], y.iloc[:train_end],\n",
    "            X.iloc[train_end:val_end], y.iloc[train_end:val_end],\n",
    "            X.iloc[val_end:], y.iloc[val_end:])\n",
    "\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return {\n",
    "        'model': name,\n",
    "        'MAE': mean_absolute_error(y_test, preds),\n",
    "        'RMSE': rmse(y_test, preds),\n",
    "        'MAPE%': mape(y_test, preds),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e83da05",
   "metadata": {},
   "source": [
    "### Explication (Imports & helpers)\n",
    "- Fonctions utilitaires partagées : RMSE(Root Mean Squared Error)/MAPE (Mean Absolute Percentage Error)  pour les métriques, split chronologique pour éviter la fuite temporelle, `evaluate_model` pour entraîner + scorer.\n",
    "- Les pipelines scaler+modèle assurent une standardisation cohérente quand nécessaire (linéaire, MLP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c4e43",
   "metadata": {},
   "source": [
    "## 1. Données et cible\n",
    "- Chargement `btc_features_daily.csv`\n",
    "- Tri chronologique\n",
    "- Cible : close du lendemain (`target_close_next`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90659d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m corr \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m       \u001b[38;5;241m.\u001b[39mcorr()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_close_next\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m       \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_close_next\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m       \u001b[38;5;241m.\u001b[39mabs()\n\u001b[1;32m      6\u001b[0m       \u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop 10 corrélations (absolues):\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(corr\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(Path('btc_features_daily.csv'), parse_dates=['timestamp']).sort_values('timestamp')\n",
    "df['target_close_next'] = df['close'].shift(-1)\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "corr = (\n",
    "    df.drop(columns=['timestamp'])\n",
    "      .corr()['target_close_next']\n",
    "      .drop('target_close_next')\n",
    "      .abs()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "print('Top 10 corrélations (absolues):')\n",
    "print(corr.head(10))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0312f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('btc_features_daily.csv')\n",
    "df = pd.read_csv(data_path, parse_dates=['timestamp']).sort_values('timestamp').reset_index(drop=True)\n",
    "print(f\"Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "print('Missing ratio (top 5):')\n",
    "print(df.isna().mean().head())\n",
    "\n",
    "# Cible = close J+1\n",
    "df['target_close_next'] = df['close'].shift(-1)\n",
    "df = df.iloc[:-1]\n",
    "feature_cols = [c for c in df.columns if c not in ['timestamp', 'target_close_next']]\n",
    "X = df[feature_cols]\n",
    "y = df['target_close_next']\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = chrono_split(X, y)\n",
    "print(f\"Splits -> train {len(X_train)}, val {len(X_val)}, test {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0eeb3",
   "metadata": {},
   "source": [
    "### Explication (Données & cible)\n",
    "- On charge `btc_features_daily.csv`, on trie par date, pas de valeurs manquantes sur les principales colonnes.\n",
    "- Cible = close du lendemain (`target_close_next`) : formulation régressive, sensible à l'autocorrélation.\n",
    "- Split 70/15/15 chronologique pour reproduire un scénario temps-réel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f78b84",
   "metadata": {},
   "source": [
    "## 2. Corrélations et contrôle de l'overfitting\n",
    "- Corrélations très élevées entre niveaux de prix (indique redondance)\n",
    "- Plan pour limiter l'overfit :\n",
    "  - Standardisation pour modèles linéaires/MLP\n",
    "  - Hyperparamètres contraints (profondeur, min_samples_leaf)\n",
    "  - Split chrono (pas de fuite temporelle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = (\n",
    "    df.drop(columns=['timestamp'])\n",
    "      .corr()['target_close_next']\n",
    "      .drop('target_close_next')\n",
    "      .abs()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "print('Top 8 corrélations absolues avec la cible:')\n",
    "print(corr.head(8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d175a5",
   "metadata": {},
   "source": [
    "### Explication (Corrélations & overfitting)\n",
    "- Les corrélations >0.99 entre prix (close/adj/typ_price) et la cible indiquent une forte redondance.\n",
    "- Risque : les modèles complexes apprennent surtout à recopier le prix actuel → difficile de généraliser.\n",
    "- Contre-mesures : régularisation, profondeur limitée, validation temporelle stricte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c173e94",
   "metadata": {},
   "source": [
    "## 3. Baseline\n",
    "Baseline persistance : prédire que le close de demain = close d’aujourd’hui.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds = X_test['close']\n",
    "baseline_res = {\n",
    "    'model': 'persistence_close',\n",
    "    'MAE': mean_absolute_error(y_test, baseline_preds),\n",
    "    'RMSE': rmse(y_test, baseline_preds),\n",
    "    'MAPE%': mape(y_test, baseline_preds),\n",
    "}\n",
    "baseline_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34362c5",
   "metadata": {},
   "source": [
    "### Explication (Baseline)\n",
    "- Cette baseline prédit simplement que le close de demain = close d'aujourd'hui.\n",
    "- Si ses MAE/RMSE/MAPE sont très bas, cela signifie que la série est fortement autocorrélée et difficile à battre.\n",
    "- Tout modèle avancé doit faire mieux (MAE/RMSE plus petits, MAPE plus faible) pour être intéressant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd00fdd",
   "metadata": {},
   "source": [
    "## 4. Modèles avancés (versions cours)\n",
    "- ElasticNet (L1+L2) pour gérer colinéarité et sélection douce.\n",
    "- Ridge (référence linéaire régularisée).\n",
    "- GradientBoostingRegressor et HistGradientBoostingRegressor (plus robuste, bins).\n",
    "- RandomForestRegressor et ExtraTreesRegressor (arbres bagging/variance réduite).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaffac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('elasticnet', make_pipeline(StandardScaler(), ElasticNet(alpha=0.1, l1_ratio=0.5, max_iter=5000))),\n",
    "    ('ridge', make_pipeline(StandardScaler(), Ridge(alpha=5.0))),\n",
    "    ('gboost', GradientBoostingRegressor(random_state=0, n_estimators=400, learning_rate=0.02, max_depth=3)),\n",
    "    ('hist_gb', HistGradientBoostingRegressor(random_state=0, max_depth=4, learning_rate=0.05, max_iter=500)),\n",
    "    ('rf', RandomForestRegressor(n_estimators=400, random_state=0, n_jobs=-1, max_depth=6, min_samples_leaf=3)),\n",
    "    ('extra_trees', ExtraTreesRegressor(n_estimators=400, random_state=0, n_jobs=-1, max_depth=6, min_samples_leaf=3)),\n",
    "]\n",
    "\n",
    "results = [baseline_res]\n",
    "for name, model in models:\n",
    "    results.append(evaluate_model(name, model, X_train, y_train, X_test, y_test))\n",
    "\n",
    "pd.DataFrame(results).sort_values('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c069990b",
   "metadata": {},
   "source": [
    "### Explication (Baseline)\n",
    "- Cette baseline prédit simplement que le close de demain = close d'aujourd'hui.\n",
    "- Si ses MAE/RMSE/MAPE sont très bas, cela signifie que la série est fortement autocorrélée et difficile à battre.\n",
    "- Tout modèle avancé doit faire mieux (MAE/RMSE plus petits, MAPE plus faible) pour être intéressant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81552138",
   "metadata": {},
   "source": [
    "### Explication (Modèles avancés)\n",
    "- **ElasticNet/Ridge** : testent si une régularisation linéaire peut extraire un léger signal malgré la colinéarité.\n",
    "- **GB/HistGB** : capturent des non-linéarités; l'apprentissage lent (petit learning_rate) limite l'overfit.\n",
    "- **RF/ExtraTrees** : bagging d'arbres, ici contraints (max_depth/min_samples_leaf) pour réduire l'overfit.\n",
    "- Interprétation : comparez leurs métriques au baseline. Si elles sont plus grandes, ils n'apportent pas de gain; s'ils sont plus petites, ils surpassent la persistance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356e1d1",
   "metadata": {},
   "source": [
    "## 5. Ensemble learning\n",
    "On combine plusieurs modèles pour lisser leurs erreurs : moyenne simple et StackingRegressor (ridge meta-learner).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fea984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit base models\n",
    "fitted = {}\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    fitted[name] = model\n",
    "\n",
    "# Moyenne simple des prédictions des 4 meilleurs modèles (ElasticNet, Ridge, RF, HistGB)\n",
    "selected = ['elasticnet','ridge','rf','hist_gb']\n",
    "preds_avg = np.mean([fitted[n].predict(X_test) for n in selected], axis=0)\n",
    "ensemble_avg = {\n",
    "    'model': 'avg_elastic_ridge_rf_histgb',\n",
    "    'MAE': mean_absolute_error(y_test, preds_avg),\n",
    "    'RMSE': rmse(y_test, preds_avg),\n",
    "    'MAPE%': mape(y_test, preds_avg),\n",
    "}\n",
    "\n",
    "# Stacking\n",
    "estimators = [(n, fitted[n]) for n in selected]\n",
    "stack = StackingRegressor(estimators=estimators, final_estimator=Ridge(alpha=1.0), passthrough=False)\n",
    "stack.fit(X_train, y_train)\n",
    "preds_stack = stack.predict(X_test)\n",
    "ensemble_stack = {\n",
    "    'model': 'stack_elastic_ridge_rf_histgb',\n",
    "    'MAE': mean_absolute_error(y_test, preds_stack),\n",
    "    'RMSE': rmse(y_test, preds_stack),\n",
    "    'MAPE%': mape(y_test, preds_stack),\n",
    "}\n",
    "\n",
    "ensemble_results = [ensemble_avg, ensemble_stack]\n",
    "pd.DataFrame(results + ensemble_results).sort_values('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfd12d",
   "metadata": {},
   "source": [
    "### Explication (Ensemble learning)\n",
    "- **Moyenne simple** : lisse les erreurs de modèles complémentaires; efficace si les erreurs ne sont pas corrélées.\n",
    "- **Stacking** : méta-modèle (Ridge) apprend à pondérer les prédictions; peut capturer des interactions entre modèles.\n",
    "- Lecture : si l'ensemble baisse MAE/RMSE/MAPE vs chaque modèle individuel et vs baseline, il apporte de la robustesse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0fc8d",
   "metadata": {},
   "source": [
    "## 6. Modèle hors cours : MLP Regressor (réseau de neurones feed-forward)\n",
    "Référence : Zhang, G. P. (2003), \"Time series forecasting using a hybrid ARIMA and neural network model\", *Neurocomputing*. Les MLP capturent des relations non linéaires; ici on l’emploie sur les features existantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85091a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=500, random_state=0, early_stopping=True)\n",
    ")\n",
    "\n",
    "mlp_res = evaluate_model('mlp_regressor', mlp, X_train, y_train, X_test, y_test)\n",
    "\n",
    "pd.DataFrame(results + ensemble_results + [mlp_res]).sort_values('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec30ba4",
   "metadata": {},
   "source": [
    "### Explication (MLP hors cours)\n",
    "- MLP Regressor (réseau de neurones feed-forward) peut modéliser des relations non linéaires complexes.\n",
    "- Early stopping réduit l'overfit; le scaling est crucial.\n",
    "- Interprétation : comparez ses métriques à la baseline et aux autres. Si l'erreur est plus élevée, les features actuelles ne suffisent pas; si elle baisse, cela suggère un signal non linéaire exploité.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc72581",
   "metadata": {},
   "source": [
    "## 7. Analyse et comparaison\n",
    "- Comparez chaque modèle au baseline persistance.\n",
    "- Notez si l’ensemble (avg/stack) ou le MLP apportent un gain.\n",
    "- Inspectez les métriques (MAE/RMSE/MAPE) et la robustesse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e83593",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = pd.DataFrame(results + ensemble_results + [mlp_res]).sort_values('RMSE')\n",
    "final_table.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1dd5c7",
   "metadata": {},
   "source": [
    "### Explication (Analyse finale)\n",
    "- Triez par RMSE/MAE : plus petit = meilleur. La MAPE donne une lecture en %.\n",
    "- Identifier le premier modèle qui bat clairement la baseline; sinon conclure que les features sont insuffisantes et qu'il faut changer la cible (retours/direction) ou ajouter des variables exogènes.\n",
    "- Vérifier la stabilité : si un modèle surperforme mais avec très faible marge, valider en rolling split/val croisée temporelle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41e467",
   "metadata": {},
   "source": [
    "### Conclusion rapide\n",
    "- Si aucun modèle ne bat le baseline : les features n’apportent pas de signal supplémentaire; envisager des cibles en rendements/direction et des variables exogènes.\n",
    "- Si un ensemble ou le MLP réduit la MAPE/MAE, poursuivre l’affinage (tuning hyperparam, rolling validation).\n",
    "- Toujours valider en split temporel ou rolling pour éviter la fuite.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
